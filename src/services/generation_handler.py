"""Generation handler for Flow2API"""
import asyncio
import base64
import json
import time
from typing import Optional, AsyncGenerator, List, Dict, Any
from ..core.logger import debug_logger
from ..core.config import config
from ..core.models import Task, RequestLog
from .file_cache import FileCache


# Model configuration
MODEL_CONFIG = {
    # å›¾ç‰‡ç”Ÿæˆ - GEM_PIX (Gemini 2.5 Flash)
    "gemini-2.5-flash-image-landscape": {
        "type": "image",
        "model_name": "GEM_PIX",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_LANDSCAPE"
    },
    "gemini-2.5-flash-image-portrait": {
        "type": "image",
        "model_name": "GEM_PIX",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_PORTRAIT"
    },

    # å›¾ç‰‡ç”Ÿæˆ - GEM_PIX_2 (Gemini 3.0 Pro)
    "gemini-3.0-pro-image-landscape": {
        "type": "image",
        "model_name": "GEM_PIX_2",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_LANDSCAPE"
    },
    "gemini-3.0-pro-image-portrait": {
        "type": "image",
        "model_name": "GEM_PIX_2",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_PORTRAIT"
    },

    # å›¾ç‰‡ç”Ÿæˆ - IMAGEN_3_5 (Imagen 4.0)
    "imagen-4.0-generate-preview-landscape": {
        "type": "image",
        "model_name": "IMAGEN_3_5",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_LANDSCAPE"
    },
    "imagen-4.0-generate-preview-portrait": {
        "type": "image",
        "model_name": "IMAGEN_3_5",
        "aspect_ratio": "IMAGE_ASPECT_RATIO_PORTRAIT"
    },

    # ========== æ–‡ç”Ÿè§†é¢‘ (T2V - Text to Video) ==========
    # ä¸æ”¯æŒä¸Šä¼ å›¾ç‰‡ï¼Œåªä½¿ç”¨æ–‡æœ¬æç¤ºè¯ç”Ÿæˆ

    # veo_3_1_t2v_fast_portrait (ç«–å±)
    # ä¸Šæ¸¸æ¨¡å‹å: veo_3_1_t2v_fast_portrait
    "veo_3_1_t2v_fast_portrait": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_3_1_t2v_fast_portrait",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": False
    },
    # veo_3_1_t2v_fast_landscape (æ¨ªå±)
    # ä¸Šæ¸¸æ¨¡å‹å: veo_3_1_t2v_fast
    "veo_3_1_t2v_fast_landscape": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_3_1_t2v_fast",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": False
    },

    # veo_2_1_fast_d_15_t2v (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_2_1_fast_d_15_t2v_portrait": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_2_1_fast_d_15_t2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": False
    },
    "veo_2_1_fast_d_15_t2v_landscape": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_2_1_fast_d_15_t2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": False
    },

    # veo_2_0_t2v (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_2_0_t2v_portrait": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_2_0_t2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": False
    },
    "veo_2_0_t2v_landscape": {
        "type": "video",
        "video_type": "t2v",
        "model_key": "veo_2_0_t2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": False
    },

    # ========== é¦–å°¾å¸§æ¨¡å‹ (I2V - Image to Video) ==========
    # æ”¯æŒ1-2å¼ å›¾ç‰‡ï¼š1å¼ ä½œä¸ºé¦–å¸§ï¼Œ2å¼ ä½œä¸ºé¦–å°¾å¸§

    # veo_3_1_i2v_s_fast_fl (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_3_1_i2v_s_fast_fl_portrait": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_3_1_i2v_s_fast_fl",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },
    "veo_3_1_i2v_s_fast_fl_landscape": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_3_1_i2v_s_fast_fl",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },

    # veo_2_1_fast_d_15_i2v (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_2_1_fast_d_15_i2v_portrait": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_2_1_fast_d_15_i2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },
    "veo_2_1_fast_d_15_i2v_landscape": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_2_1_fast_d_15_i2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },

    # veo_2_0_i2v (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_2_0_i2v_portrait": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_2_0_i2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },
    "veo_2_0_i2v_landscape": {
        "type": "video",
        "video_type": "i2v",
        "model_key": "veo_2_0_i2v",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": True,
        "min_images": 1,
        "max_images": 2
    },

    # ========== å¤šå›¾ç”Ÿæˆ (R2V - Reference Images to Video) ==========
    # æ”¯æŒå¤šå¼ å›¾ç‰‡,ä¸é™åˆ¶æ•°é‡

    # veo_3_0_r2v_fast (éœ€è¦æ–°å¢æ¨ªç«–å±)
    "veo_3_0_r2v_fast_portrait": {
        "type": "video",
        "video_type": "r2v",
        "model_key": "veo_3_0_r2v_fast",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_PORTRAIT",
        "supports_images": True,
        "min_images": 0,
        "max_images": None  # ä¸é™åˆ¶
    },
    "veo_3_0_r2v_fast_landscape": {
        "type": "video",
        "video_type": "r2v",
        "model_key": "veo_3_0_r2v_fast",
        "aspect_ratio": "VIDEO_ASPECT_RATIO_LANDSCAPE",
        "supports_images": True,
        "min_images": 0,
        "max_images": None  # ä¸é™åˆ¶
    }
}


class GenerationHandler:
    """ç»Ÿä¸€ç”Ÿæˆå¤„ç†å™¨"""

    def __init__(self, flow_client, token_manager, load_balancer, db, concurrency_manager, proxy_manager):
        self.flow_client = flow_client
        self.token_manager = token_manager
        self.load_balancer = load_balancer
        self.db = db
        self.concurrency_manager = concurrency_manager
        self.file_cache = FileCache(
            cache_dir="tmp",
            default_timeout=config.cache_timeout,
            proxy_manager=proxy_manager
        )

    async def check_token_availability(self, is_image: bool, is_video: bool) -> bool:
        """æ£€æŸ¥Tokenå¯ç”¨æ€§

        Args:
            is_image: æ˜¯å¦æ£€æŸ¥å›¾ç‰‡ç”ŸæˆToken
            is_video: æ˜¯å¦æ£€æŸ¥è§†é¢‘ç”ŸæˆToken

        Returns:
            Trueè¡¨ç¤ºæœ‰å¯ç”¨Token, Falseè¡¨ç¤ºæ— å¯ç”¨Token
        """
        token_obj = await self.load_balancer.select_token(
            for_image_generation=is_image,
            for_video_generation=is_video
        )
        return token_obj is not None

    async def handle_generation(
        self,
        model: str,
        prompt: str,
        images: Optional[List[bytes]] = None,
        stream: bool = False
    ) -> AsyncGenerator:
        """ç»Ÿä¸€ç”Ÿæˆå…¥å£

        Args:
            model: æ¨¡å‹åç§°
            prompt: æç¤ºè¯
            images: å›¾ç‰‡åˆ—è¡¨ (bytesæ ¼å¼)
            stream: æ˜¯å¦æµå¼è¾“å‡º
        """
        start_time = time.time()
        token = None

        # 1. éªŒè¯æ¨¡å‹
        if model not in MODEL_CONFIG:
            error_msg = f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}"
            debug_logger.log_error(error_msg)
            yield self._create_error_response(error_msg)
            return

        model_config = MODEL_CONFIG[model]
        generation_type = model_config["type"]
        debug_logger.log_info(f"[GENERATION] å¼€å§‹ç”Ÿæˆ - æ¨¡å‹: {model}, ç±»å‹: {generation_type}, Prompt: {prompt[:50]}...")

        # éæµå¼æ¨¡å¼: åªæ£€æŸ¥å¯ç”¨æ€§
        if not stream:
            is_image = (generation_type == "image")
            is_video = (generation_type == "video")
            available = await self.check_token_availability(is_image, is_video)

            if available:
                if is_image:
                    message = "æ‰€æœ‰Tokenå¯ç”¨äºå›¾ç‰‡ç”Ÿæˆã€‚è¯·å¯ç”¨æµå¼æ¨¡å¼ä½¿ç”¨ç”ŸæˆåŠŸèƒ½ã€‚"
                else:
                    message = "æ‰€æœ‰Tokenå¯ç”¨äºè§†é¢‘ç”Ÿæˆã€‚è¯·å¯ç”¨æµå¼æ¨¡å¼ä½¿ç”¨ç”ŸæˆåŠŸèƒ½ã€‚"
            else:
                if is_image:
                    message = "æ²¡æœ‰å¯ç”¨çš„Tokenè¿›è¡Œå›¾ç‰‡ç”Ÿæˆ"
                else:
                    message = "æ²¡æœ‰å¯ç”¨çš„Tokenè¿›è¡Œè§†é¢‘ç”Ÿæˆ"

            yield self._create_completion_response(message, is_availability_check=True)
            return

        # å‘ç”¨æˆ·å±•ç¤ºå¼€å§‹ä¿¡æ¯
        if stream:
            yield self._create_stream_chunk(
                f"âœ¨ {'è§†é¢‘' if generation_type == 'video' else 'å›¾ç‰‡'}ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨\n",
                role="assistant"
            )

        # 2. é€‰æ‹©Token
        debug_logger.log_info(f"[GENERATION] æ­£åœ¨é€‰æ‹©å¯ç”¨Token...")

        if generation_type == "image":
            token = await self.load_balancer.select_token(for_image_generation=True, model=model)
        else:
            token = await self.load_balancer.select_token(for_video_generation=True, model=model)

        if not token:
            error_msg = self._get_no_token_error_message(generation_type)
            debug_logger.log_error(f"[GENERATION] {error_msg}")
            if stream:
                yield self._create_stream_chunk(f"âŒ {error_msg}\n")
            yield self._create_error_response(error_msg)
            return

        debug_logger.log_info(f"[GENERATION] å·²é€‰æ‹©Token: {token.id} ({token.email})")

        try:
            # 3. ç¡®ä¿ATæœ‰æ•ˆ
            debug_logger.log_info(f"[GENERATION] æ£€æŸ¥Token ATæœ‰æ•ˆæ€§...")
            if stream:
                yield self._create_stream_chunk("åˆå§‹åŒ–ç”Ÿæˆç¯å¢ƒ...\n")

            if not await self.token_manager.is_at_valid(token.id):
                error_msg = "Token ATæ— æ•ˆæˆ–åˆ·æ–°å¤±è´¥"
                debug_logger.log_error(f"[GENERATION] {error_msg}")
                if stream:
                    yield self._create_stream_chunk(f"âŒ {error_msg}\n")
                yield self._create_error_response(error_msg)
                return

            # é‡æ–°è·å–token (ATå¯èƒ½å·²åˆ·æ–°)
            token = await self.token_manager.get_token(token.id)

            # 4. ç¡®ä¿Projectå­˜åœ¨
            debug_logger.log_info(f"[GENERATION] æ£€æŸ¥/åˆ›å»ºProject...")

            project_id = await self.token_manager.ensure_project_exists(token.id)
            debug_logger.log_info(f"[GENERATION] Project ID: {project_id}")

            # 5. æ ¹æ®ç±»å‹å¤„ç†
            if generation_type == "image":
                debug_logger.log_info(f"[GENERATION] å¼€å§‹å›¾ç‰‡ç”Ÿæˆæµç¨‹...")
                async for chunk in self._handle_image_generation(
                    token, project_id, model_config, prompt, images, stream
                ):
                    yield chunk
            else:  # video
                debug_logger.log_info(f"[GENERATION] å¼€å§‹è§†é¢‘ç”Ÿæˆæµç¨‹...")
                async for chunk in self._handle_video_generation(
                    token, project_id, model_config, prompt, images, stream
                ):
                    yield chunk

            # 6. è®°å½•ä½¿ç”¨
            is_video = (generation_type == "video")
            await self.token_manager.record_usage(token.id, is_video=is_video)

            # é‡ç½®é”™è¯¯è®¡æ•° (è¯·æ±‚æˆåŠŸæ—¶æ¸…ç©ºè¿ç»­é”™è¯¯è®¡æ•°)
            await self.token_manager.record_success(token.id)

            debug_logger.log_info(f"[GENERATION] âœ… ç”ŸæˆæˆåŠŸå®Œæˆ")

            # 7. è®°å½•æˆåŠŸæ—¥å¿—
            duration = time.time() - start_time
            await self._log_request(
                token.id,
                f"generate_{generation_type}",
                {"model": model, "prompt": prompt[:100], "has_images": images is not None and len(images) > 0},
                {"status": "success"},
                200,
                duration
            )

        except Exception as e:
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            debug_logger.log_error(f"[GENERATION] âŒ {error_msg}")
            if stream:
                yield self._create_stream_chunk(f"âŒ {error_msg}\n")
            if token:
                # æ£€æµ‹429é”™è¯¯ï¼Œç«‹å³ç¦ç”¨token
                if "429" in str(e) or "HTTP Error 429" in str(e):
                    debug_logger.log_warning(f"[429_BAN] Token {token.id} é‡åˆ°429é”™è¯¯ï¼Œç«‹å³ç¦ç”¨")
                    await self.token_manager.ban_token_for_429(token.id)
                else:
                    await self.token_manager.record_error(token.id)
            yield self._create_error_response(error_msg)

            # è®°å½•å¤±è´¥æ—¥å¿—
            duration = time.time() - start_time
            await self._log_request(
                token.id if token else None,
                f"generate_{generation_type if model_config else 'unknown'}",
                {"model": model, "prompt": prompt[:100], "has_images": images is not None and len(images) > 0},
                {"error": error_msg},
                500,
                duration
            )

    def _get_no_token_error_message(self, generation_type: str) -> str:
        """è·å–æ— å¯ç”¨Tokenæ—¶çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯"""
        if generation_type == "image":
            return "æ²¡æœ‰å¯ç”¨çš„Tokenè¿›è¡Œå›¾ç‰‡ç”Ÿæˆã€‚æ‰€æœ‰Tokenéƒ½å¤„äºç¦ç”¨ã€å†·å´ã€é”å®šæˆ–å·²è¿‡æœŸçŠ¶æ€ã€‚"
        else:
            return "æ²¡æœ‰å¯ç”¨çš„Tokenè¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚æ‰€æœ‰Tokenéƒ½å¤„äºç¦ç”¨ã€å†·å´ã€é…é¢è€—å°½æˆ–å·²è¿‡æœŸçŠ¶æ€ã€‚"

    async def _handle_image_generation(
        self,
        token,
        project_id: str,
        model_config: dict,
        prompt: str,
        images: Optional[List[bytes]],
        stream: bool
    ) -> AsyncGenerator:
        """å¤„ç†å›¾ç‰‡ç”Ÿæˆ (åŒæ­¥è¿”å›)"""

        # è·å–å¹¶å‘æ§½ä½
        if self.concurrency_manager:
            if not await self.concurrency_manager.acquire_image(token.id):
                yield self._create_error_response("å›¾ç‰‡å¹¶å‘é™åˆ¶å·²è¾¾ä¸Šé™")
                return

        try:
            # ä¸Šä¼ å›¾ç‰‡ (å¦‚æœæœ‰)
            image_inputs = []
            if images and len(images) > 0:
                if stream:
                    yield self._create_stream_chunk(f"ä¸Šä¼  {len(images)} å¼ å‚è€ƒå›¾ç‰‡...\n")

                # æ”¯æŒå¤šå›¾è¾“å…¥
                for idx, image_bytes in enumerate(images):
                    media_id = await self.flow_client.upload_image(
                        token.at,
                        image_bytes,
                        model_config["aspect_ratio"]
                    )
                    image_inputs.append({
                        "name": media_id,
                        "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                    })
                    if stream:
                        yield self._create_stream_chunk(f"å·²ä¸Šä¼ ç¬¬ {idx + 1}/{len(images)} å¼ å›¾ç‰‡\n")

            # è°ƒç”¨ç”ŸæˆAPI
            if stream:
                yield self._create_stream_chunk("æ­£åœ¨ç”Ÿæˆå›¾ç‰‡...\n")

            result = await self.flow_client.generate_image(
                at=token.at,
                project_id=project_id,
                prompt=prompt,
                model_name=model_config["model_name"],
                aspect_ratio=model_config["aspect_ratio"],
                image_inputs=image_inputs
            )

            # æå–URL
            media = result.get("media", [])
            if not media:
                yield self._create_error_response("ç”Ÿæˆç»“æœä¸ºç©º")
                return

            image_url = media[0]["image"]["generatedImage"]["fifeUrl"]

            # ç¼“å­˜å›¾ç‰‡ (å¦‚æœå¯ç”¨)
            local_url = image_url
            if config.cache_enabled:
                try:
                    if stream:
                        yield self._create_stream_chunk("ç¼“å­˜å›¾ç‰‡ä¸­...\n")
                    cached_filename = await self.file_cache.download_and_cache(image_url, "image")
                    local_url = f"{self._get_base_url()}/tmp/{cached_filename}"
                    if stream:
                        yield self._create_stream_chunk("âœ… å›¾ç‰‡ç¼“å­˜æˆåŠŸ,å‡†å¤‡è¿”å›ç¼“å­˜åœ°å€...\n")
                except Exception as e:
                    debug_logger.log_error(f"Failed to cache image: {str(e)}")
                    # ç¼“å­˜å¤±è´¥ä¸å½±å“ç»“æœè¿”å›,ä½¿ç”¨åŸå§‹URL
                    local_url = image_url
                    if stream:
                        yield self._create_stream_chunk(f"âš ï¸ ç¼“å­˜å¤±è´¥: {str(e)}\næ­£åœ¨è¿”å›æºé“¾æ¥...\n")
            else:
                if stream:
                    yield self._create_stream_chunk("ç¼“å­˜å·²å…³é—­,æ­£åœ¨è¿”å›æºé“¾æ¥...\n")

            # è¿”å›ç»“æœ
            if stream:
                yield self._create_stream_chunk(
                    f"ğŸ¨Finished!\n\n![Generated Image]({local_url})",
                    finish_reason="stop"
                )
            else:
                yield self._create_completion_response(
                    local_url,  # ç›´æ¥ä¼ URL,è®©æ–¹æ³•å†…éƒ¨æ ¼å¼åŒ–
                    media_type="image"
                )

        finally:
            # é‡Šæ”¾å¹¶å‘æ§½ä½
            if self.concurrency_manager:
                await self.concurrency_manager.release_image(token.id)

    async def _handle_video_generation(
        self,
        token,
        project_id: str,
        model_config: dict,
        prompt: str,
        images: Optional[List[bytes]],
        stream: bool
    ) -> AsyncGenerator:
        """å¤„ç†è§†é¢‘ç”Ÿæˆ (å¼‚æ­¥è½®è¯¢)"""

        # è·å–å¹¶å‘æ§½ä½
        if self.concurrency_manager:
            if not await self.concurrency_manager.acquire_video(token.id):
                yield self._create_error_response("è§†é¢‘å¹¶å‘é™åˆ¶å·²è¾¾ä¸Šé™")
                return

        try:
            # è·å–æ¨¡å‹ç±»å‹å’Œé…ç½®
            video_type = model_config.get("video_type")
            supports_images = model_config.get("supports_images", False)
            min_images = model_config.get("min_images", 0)
            max_images = model_config.get("max_images", 0)

            # å›¾ç‰‡æ•°é‡
            image_count = len(images) if images else 0

            # ========== éªŒè¯å’Œå¤„ç†å›¾ç‰‡ ==========

            # T2V: æ–‡ç”Ÿè§†é¢‘ - ä¸æ”¯æŒå›¾ç‰‡
            if video_type == "t2v":
                if image_count > 0:
                    if stream:
                        yield self._create_stream_chunk("âš ï¸ æ–‡ç”Ÿè§†é¢‘æ¨¡å‹ä¸æ”¯æŒä¸Šä¼ å›¾ç‰‡,å°†å¿½ç•¥å›¾ç‰‡ä»…ä½¿ç”¨æ–‡æœ¬æç¤ºè¯ç”Ÿæˆ\n")
                    debug_logger.log_warning(f"[T2V] æ¨¡å‹ {model_config['model_key']} ä¸æ”¯æŒå›¾ç‰‡,å·²å¿½ç•¥ {image_count} å¼ å›¾ç‰‡")
                images = None  # æ¸…ç©ºå›¾ç‰‡
                image_count = 0

            # I2V: é¦–å°¾å¸§æ¨¡å‹ - éœ€è¦1-2å¼ å›¾ç‰‡
            elif video_type == "i2v":
                if image_count < min_images or image_count > max_images:
                    error_msg = f"âŒ é¦–å°¾å¸§æ¨¡å‹éœ€è¦ {min_images}-{max_images} å¼ å›¾ç‰‡,å½“å‰æä¾›äº† {image_count} å¼ "
                    if stream:
                        yield self._create_stream_chunk(f"{error_msg}\n")
                    yield self._create_error_response(error_msg)
                    return

            # R2V: å¤šå›¾ç”Ÿæˆ - æ”¯æŒå¤šå¼ å›¾ç‰‡,ä¸é™åˆ¶æ•°é‡
            elif video_type == "r2v":
                # ä¸å†é™åˆ¶æœ€å¤§å›¾ç‰‡æ•°é‡
                pass

            # ========== ä¸Šä¼ å›¾ç‰‡ ==========
            start_media_id = None
            end_media_id = None
            reference_images = []

            # I2V: é¦–å°¾å¸§å¤„ç†
            if video_type == "i2v" and images:
                if image_count == 1:
                    # åªæœ‰1å¼ å›¾: ä»…ä½œä¸ºé¦–å¸§
                    if stream:
                        yield self._create_stream_chunk("ä¸Šä¼ é¦–å¸§å›¾ç‰‡...\n")
                    start_media_id = await self.flow_client.upload_image(
                        token.at, images[0], model_config["aspect_ratio"]
                    )
                    debug_logger.log_info(f"[I2V] ä»…ä¸Šä¼ é¦–å¸§: {start_media_id}")

                elif image_count == 2:
                    # 2å¼ å›¾: é¦–å¸§+å°¾å¸§
                    if stream:
                        yield self._create_stream_chunk("ä¸Šä¼ é¦–å¸§å’Œå°¾å¸§å›¾ç‰‡...\n")
                    start_media_id = await self.flow_client.upload_image(
                        token.at, images[0], model_config["aspect_ratio"]
                    )
                    end_media_id = await self.flow_client.upload_image(
                        token.at, images[1], model_config["aspect_ratio"]
                    )
                    debug_logger.log_info(f"[I2V] ä¸Šä¼ é¦–å°¾å¸§: {start_media_id}, {end_media_id}")

            # R2V: å¤šå›¾å¤„ç†
            elif video_type == "r2v" and images:
                if stream:
                    yield self._create_stream_chunk(f"ä¸Šä¼  {image_count} å¼ å‚è€ƒå›¾ç‰‡...\n")

                for idx, img in enumerate(images):  # ä¸Šä¼ æ‰€æœ‰å›¾ç‰‡,ä¸é™åˆ¶æ•°é‡
                    media_id = await self.flow_client.upload_image(
                        token.at, img, model_config["aspect_ratio"]
                    )
                    reference_images.append({
                        "imageUsageType": "IMAGE_USAGE_TYPE_ASSET",
                        "mediaId": media_id
                    })
                debug_logger.log_info(f"[R2V] ä¸Šä¼ äº† {len(reference_images)} å¼ å‚è€ƒå›¾ç‰‡")

            # ========== è°ƒç”¨ç”ŸæˆAPI ==========
            if stream:
                yield self._create_stream_chunk("æäº¤è§†é¢‘ç”Ÿæˆä»»åŠ¡...\n")

            # I2V: é¦–å°¾å¸§ç”Ÿæˆ
            if video_type == "i2v" and start_media_id:
                if end_media_id:
                    # æœ‰é¦–å°¾å¸§
                    result = await self.flow_client.generate_video_start_end(
                        at=token.at,
                        project_id=project_id,
                        prompt=prompt,
                        model_key=model_config["model_key"],
                        aspect_ratio=model_config["aspect_ratio"],
                        start_media_id=start_media_id,
                        end_media_id=end_media_id,
                        user_paygate_tier=token.user_paygate_tier or "PAYGATE_TIER_ONE"
                    )
                else:
                    # åªæœ‰é¦–å¸§
                    result = await self.flow_client.generate_video_start_image(
                        at=token.at,
                        project_id=project_id,
                        prompt=prompt,
                        model_key=model_config["model_key"],
                        aspect_ratio=model_config["aspect_ratio"],
                        start_media_id=start_media_id,
                        user_paygate_tier=token.user_paygate_tier or "PAYGATE_TIER_ONE"
                    )

            # R2V: å¤šå›¾ç”Ÿæˆ
            elif video_type == "r2v" and reference_images:
                result = await self.flow_client.generate_video_reference_images(
                    at=token.at,
                    project_id=project_id,
                    prompt=prompt,
                    model_key=model_config["model_key"],
                    aspect_ratio=model_config["aspect_ratio"],
                    reference_images=reference_images,
                    user_paygate_tier=token.user_paygate_tier or "PAYGATE_TIER_ONE"
                )

            # T2V æˆ– R2Væ— å›¾: çº¯æ–‡æœ¬ç”Ÿæˆ
            else:
                result = await self.flow_client.generate_video_text(
                    at=token.at,
                    project_id=project_id,
                    prompt=prompt,
                    model_key=model_config["model_key"],
                    aspect_ratio=model_config["aspect_ratio"],
                    user_paygate_tier=token.user_paygate_tier or "PAYGATE_TIER_ONE"
                )

            # è·å–task_idå’Œoperations
            operations = result.get("operations", [])
            if not operations:
                yield self._create_error_response("ç”Ÿæˆä»»åŠ¡åˆ›å»ºå¤±è´¥")
                return

            operation = operations[0]
            task_id = operation["operation"]["name"]
            scene_id = operation.get("sceneId")

            # ä¿å­˜Taskåˆ°æ•°æ®åº“
            task = Task(
                task_id=task_id,
                token_id=token.id,
                model=model_config["model_key"],
                prompt=prompt,
                status="processing",
                scene_id=scene_id
            )
            await self.db.create_task(task)

            # è½®è¯¢ç»“æœ
            if stream:
                yield self._create_stream_chunk(f"è§†é¢‘ç”Ÿæˆä¸­...\n")

            async for chunk in self._poll_video_result(token, operations, stream):
                yield chunk

        finally:
            # é‡Šæ”¾å¹¶å‘æ§½ä½
            if self.concurrency_manager:
                await self.concurrency_manager.release_video(token.id)

    async def _poll_video_result(
        self,
        token,
        operations: List[Dict],
        stream: bool
    ) -> AsyncGenerator:
        """è½®è¯¢è§†é¢‘ç”Ÿæˆç»“æœ"""

        max_attempts = config.max_poll_attempts
        poll_interval = config.poll_interval

        for attempt in range(max_attempts):
            await asyncio.sleep(poll_interval)

            try:
                result = await self.flow_client.check_video_status(token.at, operations)
                checked_operations = result.get("operations", [])

                if not checked_operations:
                    continue

                operation = checked_operations[0]
                status = operation.get("status")

                # çŠ¶æ€æ›´æ–° - æ¯20ç§’æŠ¥å‘Šä¸€æ¬¡ (poll_interval=3ç§’, 20ç§’çº¦7æ¬¡è½®è¯¢)
                progress_update_interval = 7  # æ¯7æ¬¡è½®è¯¢ = 21ç§’
                if stream and attempt % progress_update_interval == 0:  # æ¯20ç§’æŠ¥å‘Šä¸€æ¬¡
                    progress = min(int((attempt / max_attempts) * 100), 95)
                    yield self._create_stream_chunk(f"ç”Ÿæˆè¿›åº¦: {progress}%\n")

                # æ£€æŸ¥çŠ¶æ€
                if status == "MEDIA_GENERATION_STATUS_SUCCESSFUL":
                    # æˆåŠŸ
                    metadata = operation["operation"].get("metadata", {})
                    video_info = metadata.get("video", {})
                    video_url = video_info.get("fifeUrl")

                    if not video_url:
                        yield self._create_error_response("è§†é¢‘URLä¸ºç©º")
                        return

                    # ç¼“å­˜è§†é¢‘ (å¦‚æœå¯ç”¨)
                    local_url = video_url
                    if config.cache_enabled:
                        try:
                            if stream:
                                yield self._create_stream_chunk("æ­£åœ¨ç¼“å­˜è§†é¢‘æ–‡ä»¶...\n")
                            cached_filename = await self.file_cache.download_and_cache(video_url, "video")
                            local_url = f"{self._get_base_url()}/tmp/{cached_filename}"
                            if stream:
                                yield self._create_stream_chunk("âœ… è§†é¢‘ç¼“å­˜æˆåŠŸ,å‡†å¤‡è¿”å›ç¼“å­˜åœ°å€...\n")
                        except Exception as e:
                            debug_logger.log_error(f"Failed to cache video: {str(e)}")
                            # ç¼“å­˜å¤±è´¥ä¸å½±å“ç»“æœè¿”å›,ä½¿ç”¨åŸå§‹URL
                            local_url = video_url
                            if stream:
                                yield self._create_stream_chunk(f"âš ï¸ ç¼“å­˜å¤±è´¥: {str(e)}\næ­£åœ¨è¿”å›æºé“¾æ¥...\n")
                    else:
                        if stream:
                            yield self._create_stream_chunk("ç¼“å­˜å·²å…³é—­,æ­£åœ¨è¿”å›æºé“¾æ¥...\n")

                    # æ›´æ–°æ•°æ®åº“
                    task_id = operation["operation"]["name"]
                    await self.db.update_task(
                        task_id,
                        status="completed",
                        progress=100,
                        result_urls=[local_url],
                        completed_at=time.time()
                    )

                    # è¿”å›ç»“æœ
                    if stream:
                        yield self._create_stream_chunk(
                            f"<video src='{local_url}' controls style='max-width:100%'></video>",
                            finish_reason="stop"
                        )
                    else:
                        yield self._create_completion_response(
                            local_url,  # ç›´æ¥ä¼ URL,è®©æ–¹æ³•å†…éƒ¨æ ¼å¼åŒ–
                            media_type="video"
                        )
                    return

                elif status.startswith("MEDIA_GENERATION_STATUS_ERROR"):
                    # å¤±è´¥
                    yield self._create_error_response(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {status}")
                    return

            except Exception as e:
                debug_logger.log_error(f"Poll error: {str(e)}")
                continue

        # è¶…æ—¶
        yield self._create_error_response(f"è§†é¢‘ç”Ÿæˆè¶…æ—¶ (å·²è½®è¯¢{max_attempts}æ¬¡)")

    # ========== å“åº”æ ¼å¼åŒ– ==========

    def _create_stream_chunk(self, content: str, role: str = None, finish_reason: str = None) -> str:
        """åˆ›å»ºæµå¼å“åº”chunk"""
        import json
        import time

        chunk = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "flow2api",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": finish_reason
            }]
        }

        if role:
            chunk["choices"][0]["delta"]["role"] = role

        if finish_reason:
            chunk["choices"][0]["delta"]["content"] = content
        else:
            chunk["choices"][0]["delta"]["reasoning_content"] = content

        return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    def _create_completion_response(self, content: str, media_type: str = "image", is_availability_check: bool = False) -> str:
        """åˆ›å»ºéæµå¼å“åº”

        Args:
            content: åª’ä½“URLæˆ–çº¯æ–‡æœ¬æ¶ˆæ¯
            media_type: åª’ä½“ç±»å‹ ("image" æˆ– "video")
            is_availability_check: æ˜¯å¦ä¸ºå¯ç”¨æ€§æ£€æŸ¥å“åº” (çº¯æ–‡æœ¬æ¶ˆæ¯)

        Returns:
            JSONæ ¼å¼çš„å“åº”
        """
        import json
        import time

        # å¯ç”¨æ€§æ£€æŸ¥: è¿”å›çº¯æ–‡æœ¬æ¶ˆæ¯
        if is_availability_check:
            formatted_content = content
        else:
            # åª’ä½“ç”Ÿæˆ: æ ¹æ®åª’ä½“ç±»å‹æ ¼å¼åŒ–å†…å®¹ä¸ºMarkdown
            if media_type == "video":
                formatted_content = f"```html\n<video src='{content}' controls></video>\n```"
            else:  # image
                formatted_content = f"ğŸ¨Finished!\n\n![Generated Image]({content})"

        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "flow2api",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": formatted_content
                },
                "finish_reason": "stop"
            }]
        }

        return json.dumps(response, ensure_ascii=False)

    def _create_error_response(self, error_message: str) -> str:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        import json

        error = {
            "error": {
                "message": error_message,
                "type": "invalid_request_error",
                "code": "generation_failed"
            }
        }

        return json.dumps(error, ensure_ascii=False)

    def _get_base_url(self) -> str:
        """è·å–åŸºç¡€URLç”¨äºç¼“å­˜æ–‡ä»¶è®¿é—®"""
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„cache_base_url
        if config.cache_base_url:
            return config.cache_base_url
        # å¦åˆ™ä½¿ç”¨æœåŠ¡å™¨åœ°å€
        return f"http://{config.server_host}:{config.server_port}"

    async def _log_request(
        self,
        token_id: Optional[int],
        operation: str,
        request_data: Dict[str, Any],
        response_data: Dict[str, Any],
        status_code: int,
        duration: float
    ):
        """è®°å½•è¯·æ±‚åˆ°æ•°æ®åº“"""
        try:
            log = RequestLog(
                token_id=token_id,
                operation=operation,
                request_body=json.dumps(request_data, ensure_ascii=False),
                response_body=json.dumps(response_data, ensure_ascii=False),
                status_code=status_code,
                duration=duration
            )
            await self.db.add_request_log(log)
        except Exception as e:
            # æ—¥å¿—è®°å½•å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            debug_logger.log_error(f"Failed to log request: {e}")

